# -*- coding: utf-8 -*-
import numpy as np
import geatpy as ea
import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from geatpy.zqq.load_data import load_data
from sklearn.model_selection import KFold
import time
import os
import copy
import matplotlib.pyplot as plt


class NNAdultProblem(ea.Problem):  # 继承Problem父类
    def __init__(self, M=None, learning_rate=0.01, batch_size=500, sensitive_attributions=None,
                 epoches=2, dataname='ricci', objectives_class=None, dirname=None, preserve_sens_in_net=0,
                 seed_split_traintest=2021):
        if objectives_class is None:
            objectives_class = ['MSE', 'individual', 'group']
            M = len(objectives_class)
        if sensitive_attributions is None:
            if dataname == 'ricci':
                sensitive_attributions = ['Race']
            elif dataname == 'adult':
                sensitive_attributions = ['sex']
            elif dataname == 'german':
                sensitive_attributions = ['sex']
            else:
                print('There is no dataset called ', dataname)
        name = 'FairnessProblem'  # 初始化name（函数名称，可以随意设置）
        Dim = 5  # 初始化Dim（决策变量维数）
        maxormins = [1] * M  # 初始化maxormins（目标最小最大化标记列表，1：最小化该目标；-1：最大化该目标）
        varTypes = [1] * Dim  # 初始化varTypes（决策变量的类型，0：实数；1：整数）
        lb = [0] * Dim  # 决策变量下界
        ub = [10] * Dim  # 决策变量上界
        lbin = [1] * Dim  # 决策变量下边界（0表示不包含该变量的下边界，1表示包含）
        ubin = [1] * Dim  # 决策变量上边界（0表示不包含该变量的上边界，1表示包含）
        # 调用父类构造方法完成实例化
        ea.Problem.__init__(self, name, M, maxormins, Dim, varTypes, lb, ub, lbin, ubin)
        # [self.train_data, self.train_data_norm, self.test_data, self.test_data_norm, self.train_label,
        #  self.test_label, self.train_y, self.test_y, self.positive_y] = load_data(dataname, test_size=0.25)
        self.preserve_sens_in_net = preserve_sens_in_net
        self.seed_split_traintest = seed_split_traintest
        DATA, dataset_obj = load_data(dataname, test_size=0.25, preserve_sens_in_net=preserve_sens_in_net,
                                      seed_split_traintest=seed_split_traintest)
        self.train_data = DATA['train_data']
        self.train_data_norm = DATA['train_data_norm']
        self.train_label = DATA['train_label']
        self.train_y = DATA['train_y']

        self.valid_data = DATA['valid_data']
        self.valid_data_norm = DATA['valid_data_norm']
        self.valid_label = DATA['valid_label']
        self.valid_y = DATA['valid_y']

        self.test_data = DATA['test_data']
        self.test_data_norm = DATA['test_data_norm']
        self.test_label = DATA['test_label']
        self.test_y = DATA['test_y']

        self.data_org = DATA['org_data']
        self.train_org = DATA['train_org']
        self.valid_org = DATA['valid_org']
        self.test_org = DATA['test_org']

        self.positive_class_name = DATA['positive_class_name']
        self.positive_class = DATA['positive_class']

        self.sens_flag = DATA['sens_flag']
        self.sens_flag_name = DATA['sens_flag_name']

        # ricci : Race
        # adult : sex, race
        # german : sex, age
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.sensitive_attributions = sensitive_attributions
        self.epoches = epoches
        self.M = M
        self.num_features = self.train_data_norm.shape[1]
        self.dataname = dataname
        self.dataset_obj = dataset_obj
        self.objectives_class = objectives_class
        self.dirname = 'zqq/' + dirname

    def getFeature(self):
        return self.num_features

    def aimFunc(self, pop, kfold=0):  # 目标函数
        # kfold = 0 : 全部的train训练model
        # kfold！= 0 : 将train进行kfold并 k 为输入的数值
        start_time = time.time()
        if kfold == 0:
            # 这种情况下会修改pop中个体网络的权重值
            # -------- ZQQ - begin -----------
            popsize = len(pop)
            use_gpu = torch.cuda.is_available()
            # use_gpu = False
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            # 依次，accuracy MSE L2 Individual Group
            AllObj_valid = np.zeros([popsize, 5])
            AllObj_test = np.zeros([popsize, 5])
            AllObj_train = np.zeros([popsize, 5])

            pop_logits_test = np.zeros([popsize, self.test_data_norm.shape[0]])
            # record_loss = np.zeros([popsize, self.epoches*10])
            # record_test = np.zeros([popsize, self.epoches*10])
            # Groups_info = []

            # for ti in range(10):
            #     print(ti)
            for idx in range(popsize):

                individual = pop.Chrom[idx]  # 只是引用，不是复制，还会修改pop.Chrom 网络的值
                # individual = copy.deepcopy(pop.Chrom[idx])  # 不是引用，是复制，不会修改pop.Chrom 网络的值

                if use_gpu:
                    individual.cuda()

                x_train = torch.Tensor(self.train_data_norm)
                y_train = torch.Tensor(np.array(self.train_y))
                y_train = y_train.view(y_train.shape[0], 1)

                x_test = torch.Tensor(self.test_data_norm)
                y_test = torch.Tensor(np.array(self.test_y))
                y_test = y_test.view(y_test.shape[0], 1)

                x_valid = torch.Tensor(self.valid_data_norm)
                y_valid = torch.Tensor(self.valid_y)
                y_valid = y_valid.view(y_valid.shape[0], 1)

                optimizer = torch.optim.Adam(individual.parameters(), lr=self.learning_rate, weight_decay=1e-1)
                loss_fn = torch.nn.BCEWithLogitsLoss()  # Combined with the sigmoid

                if use_gpu:
                    loss_fn.cuda()

                train = TensorDataset(x_train, y_train)
                train_loader = DataLoader(train, batch_size=self.batch_size, shuffle=True)
                # loss_indi = np.zeros([1, self.epoches])
                # test_indi = np.zeros([1, self.epoches])
                for epoch in range(self.epoches):
                    individual.train()
                    avg_loss = 0.
                    for i, (x_batch, y_batch) in enumerate(train_loader):
                        if use_gpu:
                            x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                        y_pred = individual(x_batch)
                        loss = loss_fn(y_pred, y_batch)
                        optimizer.zero_grad()  # clear gradients for next train
                        loss.backward()  # -> accumulates the gradient (by addition) for each parameter
                        optimizer.step()  # -> update weights and biases
                        avg_loss += loss.item() / len(train_loader)
                    # loss_indi[0][epoch] = avg_loss
                    # with torch.no_grad():
                    #     logits_test = ea.sigmoid(np.array(individual(x_test).detach()))
                    #     accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test, Groups_test = ea.Cal_objectives(
                    #         self.test_data,
                    #         self.test_data_norm,
                    #         logits_test, y_test,
                    #         self.sensitive_attributions,
                    #         2)
                    # test_indi[0][epoch] = accuracy_test
                # record_loss[idx, (ti*self.epoches):(epoch+ti*self.epoches+1)] = loss_indi
                # record_test[idx, (ti*self.epoches):(epoch+ti*self.epoches+1)] = test_indi
                # mutOper = ea.Mutation_NN(mu=0, var=0.05)
                # pop.Chrom = mutOper.do(pop.Chrom)
                with torch.no_grad():
                    # if use_gpu:
                    #     individual.cuda()
                    #     x_test, x_train, x_valid = x_test.cuda(), x_train.cuda(), x_valid.cuda()
                    if use_gpu:
                        individual.cpu()
                    l2_regularization = 0.0
                    for param in individual.parameters():
                        # l1_regularization += torch.norm(param, 1)  # L1正则化
                        # print(param)
                        l2_regularization += torch.norm(param, 2)  # L2 正则化

                    logits_test = ea.sigmoid(np.array(individual(x_test).detach()))
                    pop_logits_test[idx][:] = logits_test.reshape(1, -1)
                    # accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test, Groups_test
                    Infos = ea.Cal_objectives(
                        self.test_data,
                        self.test_data_norm,
                        logits_test, y_test,
                        self.sensitive_attributions,
                        alpha=2, obj_names=['Error', "Individual_fairness", 'Accuracy'])
                    accuracy_loss_test = Infos['Error']
                    individual_fairness_test = Infos['Individual_fairness']
                    accuracy_test = Infos['Accuracy']
                    group_fairness_test = 0

                    logits_train = ea.sigmoid(np.array(individual(x_train).detach()))
                    # accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train, Groups_train = \
                    Infos = ea.Cal_objectives(
                        self.train_data,
                        self.train_data_norm,
                        logits_train, y_train,
                        self.sensitive_attributions,
                        alpha=2, obj_names=['Error', "Individual_fairness", 'Accuracy'])
                    accuracy_loss_train = Infos['Error']
                    individual_fairness_train = Infos['Individual_fairness']
                    accuracy_train = Infos['Accuracy']
                    group_fairness_train = 0

                    logits_valid = ea.sigmoid(np.array(individual(x_valid).detach()))
                    # accuracy_valid, accuracy_loss_valid, individual_fairness_valid, group_fairness_valid, Groups_valid =\
                    Infos = ea.Cal_objectives(
                        self.valid_data,
                        self.valid_data_norm,
                        logits_valid, y_valid,
                        self.sensitive_attributions,
                        alpha=2, obj_names=['Error', "Individual_fairness", 'Accuracy'])
                    accuracy_loss_valid = Infos['Error']
                    individual_fairness_valid = Infos['Individual_fairness']
                    accuracy_valid = Infos['Accuracy']
                    group_fairness_valid = 0

                    # print('all is ok')
                    ####################################################################################################
                    # # in test data
                    # print('The information in test data: ')
                    # print('  accuracy: %.4f, MSE: %.4f individual fairness: %.5f, group fairness: %.5f\n'
                    #       % (accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test))
                    #
                    # ####################################################################################################
                    # # in train data
                    # print('The formation in train data: ')
                    # print('  accuracy: %.4f, MSE: %.4f, individual fairness: %.5f, group fairness: %.5f\n'
                    #       % (accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train))
                    ####################################################################################################

                    # Groups_info.append(Groups_train)
                    #
                    AllObj_train[idx][:] = np.array(
                        [accuracy_train, accuracy_loss_train, l2_regularization, individual_fairness_train, group_fairness_train])
                    AllObj_test[idx][:] = np.array(
                        [accuracy_test, accuracy_loss_test, l2_regularization, individual_fairness_test, group_fairness_test])
                    AllObj_valid[idx][:] = np.array(
                        [accuracy_valid, accuracy_loss_valid, l2_regularization, individual_fairness_valid, group_fairness_valid])

                    # AllObj_test[idx][:] = np.array([accuracy_loss_test, individual_fairness_train, group_fairness_train])
                    # PopObj[idx][:] = np.array([individual_fairness, group_fairness])

            # mean_loss = np.mean(record_loss, axis=0)
            # mean_acc = np.mean(record_test, axis=0)
            # np.savetxt("compas_wd2_loss_lr0001.txt", record_loss)
            # np.savetxt("compas_wd2_acc_lr0001.txt", record_test)
            # plt.plot(np.array(range(self.epoches*10)), mean_loss)
            # plt.plot(np.array(range(self.epoches*10)), mean_acc)
            # end_time = time.time()
            # print('cost: ', end_time-start_time)
            # plt.show()
            # print('draw')
        else:
            # 进行 kfold，计算在valid的值作为目标值，注意，不修改原网络权重
            begin_time = time.time()
            popsize = len(pop)
            All_logits = np.array([])
            AllObj_valid = np.zeros([popsize, 4])
            AllObj_test = np.zeros([popsize, 4])
            AllObj_train = np.zeros([popsize, 4])
            Groups_info = []
            pop_logits_test = np.zeros([popsize, self.test_data_norm.shape[0]])
            use_gpu = torch.cuda.is_available()
            use_gpu = False
            for idx in range(popsize):
                kf = KFold(n_splits=kfold)
                now_k = 0
                indivi_pop_valid = np.zeros([kfold, 4])
                indivi_pop_test = np.zeros([kfold, 4])
                indivi_pop_train = np.zeros([kfold, 4])
                temp_pop_logits_test = np.zeros([kfold, self.test_data_norm.shape[0]])
                for train_index, valid_index in kf.split(self.train_data_norm):
                    pop_copy = pop.copy()  # 已在Population.py 中设置 copy() 属于copy.deepcopy()
                    individual = pop_copy.Chrom[idx]  # 训练后，individual 共用 pop_copy 网络参数，但不会修改pop中的网络参数
                    if use_gpu:
                        individual.cuda()
                    x_train = torch.Tensor(np.array(self.train_data)[train_index])
                    x_train_norm = torch.Tensor(np.array(self.train_data_norm)[train_index])
                    y_train = torch.Tensor(np.array(self.train_y)[train_index]).view(len(train_index), 1)

                    x_valid = torch.Tensor(np.array(self.train_data)[valid_index])
                    x_valid_norm = torch.Tensor(np.array(self.train_data_norm)[valid_index])
                    y_valid = torch.Tensor(np.array(self.train_y)[valid_index]).view(len(valid_index), 1)

                    x_test = torch.Tensor(self.test_data_norm)
                    y_test = torch.Tensor(np.array(self.test_y)).view(self.test_data_norm.shape[0], 1)

                    optimizer = torch.optim.Adam(individual.parameters(), lr=self.learning_rate, weight_decay=1e-5)
                    loss_fn = torch.nn.BCEWithLogitsLoss()  # Combined with the sigmoid

                    if use_gpu:
                        loss_fn.cuda()

                    train = TensorDataset(x_train_norm, y_train)
                    train_loader = DataLoader(train, batch_size=self.batch_size, shuffle=True)
                    for epoch in range(self.epoches):
                        individual.train()
                        avg_loss = 0.
                        for i, (x_batch, y_batch) in enumerate(train_loader):
                            if use_gpu:
                                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                            y_pred = individual(x_batch)

                            loss = loss_fn(y_pred, y_batch)
                            optimizer.zero_grad()  # clear gradients for next train
                            loss.backward()  # -> accumulates the gradient (by addition) for each parameter
                            optimizer.step()  # -> update weights and biases
                            avg_loss += loss.item() / len(train_loader)

                    with torch.no_grad():
                        if use_gpu:
                            individual.cpu()
                        ################################################################################################
                        # in train data
                        logits_train = ea.sigmoid(np.array(individual(x_train_norm).detach()))
                        accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train, Groups_train = ea.Cal_objectives(
                            self.train_data.iloc[train_index],
                            x_train_norm,
                            logits_train, y_train,
                            self.sensitive_attributions,
                            alpha=2, obj_names=['Error', "Individual_fairness"])

                        ################################################################################################
                        # in valid data
                        logits_valid = ea.sigmoid(np.array(individual(x_valid_norm).detach()))
                        accuracy_valid, accuracy_loss_valid, individual_fairness_valid, group_fairness_valid, Groups_valid = ea.Cal_objectives(
                            self.train_data.iloc[valid_index],
                            x_valid_norm,
                            logits_valid, y_valid,
                            self.sensitive_attributions,
                            alpha=2, obj_names=['Error', "Individual_fairness"])

                        ################################################################################################
                        # in test data
                        logits_test = ea.sigmoid(np.array(individual(x_test).detach()))
                        accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test, Groups_test = ea.Cal_objectives(
                            self.test_data,
                            self.test_data_norm,
                            logits_test, y_test,
                            self.sensitive_attributions,
                            alpha=2, obj_names=['Error', "Individual_fairness"])

                        ################################################################################################
                        # print('The formation in train data: ')
                        # print('  accuracy: %.4f, MSE: %.4f, individual fairness: %.5f, group fairness: %.5f'
                        #       % (
                        #       accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train))
                        #
                        # print('The information in valid data: ')
                        # print('  accuracy: %.4f, MSE: %.4f individual fairness: %.5f, group fairness: %.5f'
                        #       % (accuracy_valid, accuracy_loss_valid, individual_fairness_valid, group_fairness_valid))
                        #
                        # print('The information in test data: ')
                        # print('  accuracy: %.4f, MSE: %.4f individual fairness: %.5f, group fairness: %.5f'
                        #       % (accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test))
                        ################################################################################################

                    temp_pop_logits_test[now_k, :] = ea.sigmoid(np.array(individual(x_test).detach())).reshape(1, -1)

                    indivi_pop_train[now_k, :] = np.array(
                        [accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train])
                    indivi_pop_valid[now_k, :] = np.array(
                        [accuracy_valid, accuracy_loss_valid, individual_fairness_valid, group_fairness_valid])
                    indivi_pop_test[now_k, :] = np.array(
                        [accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test])
                    now_k += 1

                Groups_info.append(Groups_train)

                AllObj_valid[idx][:] = np.mean(indivi_pop_valid, axis=0)
                AllObj_test[idx][:] = np.mean(indivi_pop_test, axis=0)
                AllObj_train[idx][:] = np.mean(indivi_pop_train, axis=0)

                pop_logits_test[idx][:] = np.mean(temp_pop_logits_test, axis=0)
                # PopObj[idx][:] = np.array([individual_fairness, group_fairness])

        pop.CV = np.zeros([popsize, 1])
        PopObj = AllObj_valid.copy()
        delete_list = [0]
        if 'l2' not in self.objectives_class:
            delete_list.append(2)
        if 'Error' not in self.objectives_class:
            delete_list.append(1)
        if 'individual' not in self.objectives_class:
            delete_list.append(3)
        if 'group' not in self.objectives_class:
            delete_list.append(4)

        pop.ObjV = np.delete(PopObj, delete_list, 1)  # 把求得的目标函数值赋值给种群pop的ObjV
        pop.ObjV_train = AllObj_train
        pop.ObjV_valid = AllObj_valid
        pop.ObjV_test = AllObj_test

        endtime = time.time()
        # if use_gpu:
        #     print('use gpu')
        # else:
        #     print('not use gpu')
        # print('calculate objectives run time:', endtime - start_time)

        return AllObj_train, AllObj_valid, AllObj_test, pop_logits_test

    # 在 kfold != 0 中 AllObj_train（多次平均）, AllObj_valid（多次平均）, AllObj_test（多次平均）, 就是所对应的意思
    # 在 kfold = 0  中 AllObj_train = AllObj_valid, AllObj_test

    def train_nets(self, pop, epoches):
        # 将pop中的所有网络在train data上训练epoch遍，
        # 1. 会修改网络权重
        # 2. 返回在test与train上的四个值
        begin_time = time.time()
        popsize = len(pop)
        use_gpu = torch.cuda.is_available()
        use_gpu = False
        AllObj_test = np.zeros([popsize, 4])
        AllObj_train = np.zeros([popsize, 4])
        pop_logits_test = np.zeros([popsize, self.test_data_norm.shape[0]])
        for idx in range(popsize):
            individual = pop.Chrom[idx]  # 只是引用，不是复制，还会修改pop.Chrom 网络的值
            # individual = copy.deepcopy(pop.Chrom[idx])  # 不是引用，是复制，不会修改pop.Chrom 网络的值
            x_train = torch.Tensor(self.train_data_norm)
            y_train = torch.Tensor(np.array(self.train_y)).view(self.train_data.shape[0], 1)
            if use_gpu:
                individual.cuda()
            x_test = torch.Tensor(self.test_data_norm)
            y_test = torch.Tensor(np.array(self.test_y)).view(self.test_data_norm.shape[0], 1)

            optimizer = torch.optim.Adam(individual.parameters(), lr=self.learning_rate, weight_decay=1e-5)
            loss_fn = torch.nn.BCEWithLogitsLoss()  # Combined with the sigmoid
            if use_gpu:
                loss_fn.cuda()

            train = TensorDataset(x_train, y_train)
            train_loader = DataLoader(train, batch_size=self.batch_size, shuffle=True)
            for epoch in range(epoches):
                individual.train()
                avg_loss = 0.
                for i, (x_batch, y_batch) in enumerate(train_loader):
                    if use_gpu:
                        x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                    y_pred = individual(x_batch)
                    loss = loss_fn(y_pred, y_batch)

                    optimizer.zero_grad()  # clear gradients for next train
                    loss.backward()  # -> accumulates the gradient (by addition) for each parameter
                    optimizer.step()  # -> update weights and biases
                    avg_loss += loss.item() / len(train_loader)

            with torch.no_grad():
                if use_gpu:
                    individual.cpu()
                pop_logits_test[idx][:] = ea.sigmoid(np.array(individual(x_test).detach())).reshape(1, -1)

                logits_test = ea.sigmoid(np.array(individual(x_test).detach()))
                accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test, Groups_test = ea.Cal_objectives(
                    self.test_data,
                    self.test_data_norm,
                    logits_test, y_test,
                    self.sensitive_attributions,
                    alpha=2, obj_names=['Error', "Individual_fairness"])

                logits_train = ea.sigmoid(np.array(individual(x_train).detach()))
                accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train, Groups_train = ea.Cal_objectives(
                    self.train_data,
                    self.train_data_norm,
                    logits_train, y_train,
                    self.sensitive_attributions,
                    alpha=2, obj_names=['Error', "Individual_fairness"])
                ####################################################################################################
                # # in test data
                # print('The information in test data: ')
                # print('  accuracy: %.4f, MSE: %.4f individual fairness: %.5f, group fairness: %.5f\n'
                #       % (accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test))
                #
                # ####################################################################################################
                # # in train data
                # print('The formation in train data: ')
                # print('  accuracy: %.4f, MSE: %.4f, individual fairness: %.5f, group fairness: %.5f\n'
                #       % (accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train))
                ####################################################################################################
                AllObj_train[idx][:] = np.array(
                    [accuracy_train, accuracy_loss_train, individual_fairness_train, group_fairness_train])
                AllObj_test[idx][:] = np.array(
                    [accuracy_test, accuracy_loss_test, individual_fairness_test, group_fairness_test])
        end_time = time.time()
        # print('full train time: ', end_time-begin_time)
        return AllObj_train, AllObj_test, pop_logits_test


