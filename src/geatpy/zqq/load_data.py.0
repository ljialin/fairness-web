"""
测试数据导入
https://github.com/JSGoedhart/fairness-comparison/tree/df503855bfc2eeb9c89c1075b661bf8de5e6d18c
"""
from geatpy.zqq.data.objects.list import DATASETS, get_dataset_names
from geatpy.zqq.data.objects.ProcessedData import ProcessedData
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import StratifiedShuffleSplit
import pandas as pd
import geatpy as ea
import numpy as np
from itertools import product


def make_class_attr_num(dataframe, positive_val):
    dataframe = dataframe.replace({positive_val: 1})
    dataframe = dataframe.replace("[^1]", 0, regex=True)
    return dataframe


supported_tags = ["original", "numerical", "numerical-binsensitive",
                  "categorical-binsensitive", "numerical-for-NN", "original_info"]


def load_data(dataname, datatype="numerical-for-NN", test_size=0.25, preserve_sens_in_net=0, seed_split_traintest=2021):
    available_datasets = get_dataset_names()
    if dataname not in available_datasets:
        print('There is no dataset named', dataname)
        return None

    for dataset in DATASETS:
        # test_labels and test_y are all ones and zeros
        if dataset.dataset_name == dataname:
            processed_dataset = ProcessedData(dataset)
            data = processed_dataset.create_train_test_splits()
            data = data[datatype]

            label_name = dataset.get_class_attribute()
            data.reset_index(inplace=True, drop=True)
            data_x = data.drop(columns=label_name)
            data_label = data[label_name]

            temp = data_label.copy()
            temp.loc[(data_label != 1)] = dataset.get_negative_class_val("")
            temp.loc[(data_label == 1)] = dataset.get_positive_class_val("")
            data_label = temp
            org_data = processed_dataset.get_orig_data()

            # Train - Test split
            # train_data, test_data, train_label, test_label = train_test_split(data_x, data_label,
            #                                                                   test_size=test_size,
            #                                                                   random_state=seed_split_traintest)

            if dataset.dataset_name == 'adult':
                Sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0)
                Plan1, Plan2 = Sss.split(data_x, data_label)
                org_data = org_data.iloc[Plan1[1]]
                data_x = data_x.loc[Plan1[1]]
                data_label = data_label.loc[Plan1[1]]
                org_data.reset_index(inplace=True, drop=True)
                data_x.reset_index(inplace=True, drop=True)
                data_label.reset_index(inplace=True, drop=True)

            sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)
            plan1, plan2 = sss1.split(data_x, data_label)

            test_org = org_data.iloc[plan1[1]]
            test_data = data_x.loc[plan1[1]]
            test_label = data_label.loc[plan1[1]]
            test_data.reset_index(inplace=True, drop=True)
            test_label.reset_index(inplace=True, drop=True)
            test_org.reset_index(inplace=True, drop=True)

            trainvaliddata_org = org_data.iloc[plan1[0]]
            trainvaliddata = data_x.loc[plan1[0]]
            trainvalidlabel = data_label.loc[plan1[0]]
            trainvaliddata.reset_index(inplace=True, drop=True)
            trainvalidlabel.reset_index(inplace=True, drop=True)
            trainvaliddata_org.reset_index(inplace=True, drop=True)

            sss2 = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=0)
            plan3, plan4 = sss2.split(trainvaliddata, trainvalidlabel)

            train_org = trainvaliddata_org.loc[plan3[0]]
            train_data = trainvaliddata.loc[plan3[0]]
            train_label = trainvalidlabel.loc[plan3[0]]
            valid_org = trainvaliddata_org.loc[plan3[1]]
            valid_data = trainvaliddata.loc[plan3[1]]
            valid_label = trainvalidlabel.loc[plan3[1]]

            # if dataset.dataset_name == 'adult':
            #     try:
            #         train_data = pd.read_csv('geatpy/zqq/data/AdultData/adult2021/train_data.csv', index_col=0)
            #         test_data = pd.read_csv('geatpy/zqq/data/AdultData/adult2021/test_data.csv', index_col=0)
            #         train_label = pd.read_csv('geatpy/zqq/data/AdultData/adult2021/train_label.csv', index_col=0)
            #         test_label = pd.read_csv('geatpy/zqq/data/AdultData/adult2021/test_label.csv', index_col=0)
            #         train_label = train_label['income-per-year']
            #         test_label = test_label['income-per-year']
            #     except:
            #         train_data = pd.read_csv('zqq/data/AdultData/adult2021/train_data.csv', index_col=0)
            #         test_data = pd.read_csv('zqq/data/AdultData/adult2021/test_data.csv', index_col=0)
            #         train_label = pd.read_csv('zqq/data/AdultData/adult2021/train_label.csv', index_col=0)
            #         test_label = pd.read_csv('zqq/data/AdultData/adult2021/test_label.csv', index_col=0)
            #         train_label = train_label['income-per-year']
            #         test_label = test_label['income-per-year']

            # if dataset.dataset_name == 'propublica-recidivism':
            #     try:
            #         train_data = pd.read_csv('geatpy/zqq/data/COMPAS/compas2021/train_data.csv', index_col=0)
            #         test_data = pd.read_csv('geatpy/zqq/data/COMPAS/compas2021/test_data.csv', index_col=0)
            #         train_label = pd.read_csv('geatpy/zqq/data/COMPAS/compas2021/train_label.csv', index_col=0)
            #         test_label = pd.read_csv('geatpy/zqq/data/COMPAS/compas2021/test_label.csv', index_col=0)
            #         train_label = train_label['two_year_recid']
            #         test_label = test_label['two_year_recid']
            #     except:
            #         train_data = pd.read_csv('zqq/data/COMPAS/compas2021/train_data.csv', index_col=0)
            #         test_data = pd.read_csv('zqq/data/COMPAS/compas2021/test_data.csv', index_col=0)
            #         train_label = pd.read_csv('zqq/data/COMPAS/compas2021/train_label.csv', index_col=0)
            #         test_label = pd.read_csv('zqq/data/COMPAS/compas2021/test_label.csv', index_col=0)
            #         train_label = train_label['two_year_recid']
            #         test_label = test_label['two_year_recid']

            train_org.reset_index(inplace=True, drop=True)
            train_data.reset_index(inplace=True, drop=True)
            train_label.reset_index(inplace=True, drop=True)
            valid_data.reset_index(inplace=True, drop=True)
            valid_label.reset_index(inplace=True, drop=True)
            valid_org.reset_index(inplace=True, drop=True)

            # 是否删除sensitive attribution
            if preserve_sens_in_net == 1:
                newtrain_data = train_data.copy()
                newtest_data = test_data.copy()
                newvalid_data = valid_data.copy()
            else:
                attribution = train_data.columns
                sensitive_attributions = dataset.get_sensitive_attributes()
                sens_dict = []
                for sens in sensitive_attributions:
                    for attr in attribution:
                        if sens in attr:
                            sens_dict.append(attr)
                newtrain_data = train_data.copy()
                newtest_data = test_data.copy()
                newvalid_data = valid_data.copy()
                newdata_x = data_x.copy()
                newtrain_data.drop(columns=sens_dict, inplace=True)
                newtest_data.drop(columns=sens_dict, inplace=True)
                newvalid_data.drop(columns=sens_dict, inplace=True)
                newdata_x.drop(columns=sens_dict, inplace=True)

            # Normalization
            normalize = StandardScaler()

            # Fitting only on training data
            normalize.fit(newdata_x)
            train_data_norm = normalize.transform(newtrain_data)

            # Applying same transformation to test data
            test_data_norm = normalize.transform(newtest_data)

            # Applying same transformation to validation data
            valid_data_norm = normalize.transform(newvalid_data)

            # Change labels into integer
            train_y = make_class_attr_num(train_label.copy(), dataset.get_positive_class_val(""))
            test_y = make_class_attr_num(test_label.copy(), dataset.get_positive_class_val(""))
            valid_y = make_class_attr_num(valid_label.copy(), dataset.get_positive_class_val(""))

            DATA_names = ['train_data', 'train_data_norm', 'train_label', 'train_y', 'train_org',
                          'valid_data', 'valid_data_norm', 'valid_label', 'valid_y', 'valid_org',
                          'test_data', 'test_data_norm', 'test_label', 'test_y', 'test_org'
                          'org_data', 'positive_class', 'positive_class_name',
                          'sens_flag', 'sens_flag_name']

            DATA = dict((k, []) for k in DATA_names)

            DATA['train_data'] = train_data
            DATA['train_data_norm'] = train_data_norm
            DATA['train_label'] = train_label
            DATA['train_y'] = train_y

            DATA['valid_data'] = valid_data
            DATA['valid_data_norm'] = valid_data_norm
            DATA['valid_label'] = valid_label
            DATA['valid_y'] = valid_y

            DATA['test_data'] = test_data
            DATA['test_data_norm'] = test_data_norm
            DATA['test_label'] = test_label
            DATA['test_y'] = test_y

            DATA['positive_class'] = dataset.get_positive_class_val("")


            # train_idx = train_data.index.to_list()
            # valid_idx = valid_data.index.to_list()
            # test_idx = test_data.index.to_list()
            #
            # train_org = org_data.iloc[train_idx]
            # test_org = org_data.iloc[test_idx]
            DATA['org_data'] = org_data
            DATA['train_org'] = train_org
            DATA['test_org'] = test_org
            DATA['valid_org'] = valid_org

            DATA['positive_class_name'] = dataset.get_class_attribute()
            data_obj = dataset

            # a, b = detect_allgroups(data, dataset)
            # for i in b:
            #     print(i, len(a[i][0]))

            # 记录class：flag，如 [white:[1,0,1,1], balck:[1,1,..], female_white:[1,0,1,1], female_balck:[1,1,..].......]
            attribution = data.columns
            sensitive_attributions = dataset.get_sensitive_attributes()
            group_dict = {}
            sens_flag_name = []
            test_num = test_data.shape[0]
            sens_flag = {}
            # sens_flag_name = []
            for sens in sensitive_attributions:
                temp = []
                for attr in attribution:

                    if sens in attr:
                        temp.append(attr)
                        sens_flag.update({attr: np.array(np.where(test_data[attr])).reshape(1, -1)})
                        sens_flag_name.append(attr)

                group_dict.update({sens: temp})

            group_attr = []
            for sens in sensitive_attributions:
                group_attr.append(group_dict[sens])

            for item in product(*eval(str(group_attr))):
                group = item
                flag = np.ones([1, test_num]) == np.ones([1, test_num])
                for g in group:
                    flag = flag & test_data[g]
                g_num = np.sum(flag)
                if g_num != 0:
                    name = ("+").join(str(x) for x in group)
                    # sens_flag_name.append(name)
                    g_idx = np.array(np.where(flag)).reshape([1, g_num])
                    sens_flag.update({name: g_idx})
                    sens_flag_name.append(name)

            DATA['sens_flag'] = sens_flag
            DATA['sens_flag_name'] = sens_flag_name

            return DATA, data_obj


# train_data, train_data_norm, test_data, test_data_norm, train_label, test_label, train_y, test_y, pos_val = load_data('adult', test_size=0.25)

# DATA = load_data('adult', test_size=0.25)
# print('over')

"""
ricci
adult
german
"""
